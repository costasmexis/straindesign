{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "from config import *\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Read data & split to train / test '''\n",
    "data_A = pd.read_csv('data/data_A.csv', index_col=0)\n",
    "data_B = pd.read_csv('data/data_B.csv', index_col=0)\n",
    "data = pd.concat([data_A, data_B], axis=0)\n",
    "\n",
    "X_train = data_A[INPUT_VARS]\n",
    "y_train = data_A[RESPONSE_VARS]\n",
    "X_test = data_B[INPUT_VARS]\n",
    "y_test = data_B[RESPONSE_VARS]\n",
    "\n",
    "# # Split train to train / validation\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=32)\n",
    "\n",
    "# print(f'X_train shape: {X_train.shape} \\nX_val shape: {X_val.shape} \\nX_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OMLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omlt import OmltBlock, OffsetScaling\n",
    "from omlt.io.keras import load_keras_sequential\n",
    "from omlt.neuralnet import ReluBigMFormulation, FullSpaceSmoothNNFormulation\n",
    "import pyomo.environ as pyo\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfin = X_train\n",
    "dfout = y_train\n",
    "\n",
    "inputs = INPUT_VARS\n",
    "outputs = RESPONSE_VARS\n",
    "\n",
    "x_offset, x_factor = dfin.mean().to_dict(), dfin.std().to_dict()\n",
    "y_offset, y_factor = dfout.mean().to_dict(), dfout.std().to_dict()\n",
    "\n",
    "dfin = (dfin - dfin.mean()).divide(dfin.std())\n",
    "dfout = (dfout - dfout.mean()).divide(dfout.std())\n",
    "\n",
    "x = dfin.values\n",
    "y = dfout.values\n",
    "\n",
    "# capture the minimum and maximum values of the scaled inputs so we don't use the model outside the valid range\n",
    "scaled_lb = dfin.min()[inputs].values\n",
    "scaled_ub = dfin.max()[inputs].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our Keras Sequential model\n",
    "nn = Sequential(name='ANN')\n",
    "nn.add(Dense(units=516, input_dim=len(inputs), activation='relu'))\n",
    "nn.add(Dense(1))\n",
    "nn.compile(optimizer=Adam(), loss='mean_absolute_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "history = nn.fit(x, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get X_val transformed to ANN input\n",
    "# x_val = np.array((X_val - x_offset).divide(x_factor))\n",
    "\n",
    "# y_pred = nn.predict(x_val)\n",
    "\n",
    "# # Get y_val transformed to ANN output\n",
    "# y_pred = y_pred * y_factor['Limonene'] + y_offset['Limonene'] \n",
    "\n",
    "# # Create df with y_pred and y_val columns\n",
    "# df = pd.DataFrame({'y_pred': y_pred.flatten(), 'y_val': y_val.values.flatten()})\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATOB_ECOLI: 0.021899999999999975\n",
      "ERG8_YEAST: 0.18989999999999896\n",
      "IDI_ECOLI: 3.1812999999999985\n",
      "KIME_YEAST: 0.0983000000000008\n",
      "MVD1_YEAST: 0.42150000000000065\n",
      "Q40322_MENSP: 11.203699999999998\n",
      "Q8LKJ3_ABIGR: 4.526700000000001\n",
      "Q9FD86_STAAU: 0.027999999999999803\n",
      "Q9FD87_STAAU: 0.042899999999999716\n",
      "Limonene:  173.93197672382112\n"
     ]
    }
   ],
   "source": [
    "# first, create the Pyomo model\n",
    "m = pyo.ConcreteModel()\n",
    "# create the OmltBlock to hold the neural network model\n",
    "m.reformer = OmltBlock()\n",
    "# load the Keras model\n",
    "nn_reformer = nn\n",
    "\n",
    "# Note: The neural network is in the scaled space. We want access to the\n",
    "# variables in the unscaled space. Therefore, we need to tell OMLT about the\n",
    "# scaling factors\n",
    "scaler = OffsetScaling(\n",
    "        offset_inputs={i: x_offset[inputs[i]] for i in range(len(inputs))},\n",
    "        factor_inputs={i: x_factor[inputs[i]] for i in range(len(inputs))},\n",
    "        offset_outputs={i: y_offset[outputs[i]] for i in range(len(outputs))},\n",
    "        factor_outputs={i: y_factor[outputs[i]] for i in range(len(outputs))}\n",
    "    )\n",
    "\n",
    "scaled_input_bounds = {i: (scaled_lb[i], scaled_ub[i]) for i in range(len(inputs))}\n",
    "\n",
    "# create a network definition from the Keras model\n",
    "net = load_keras_sequential(nn_reformer, scaling_object=scaler, scaled_input_bounds=scaled_input_bounds)\n",
    "\n",
    "# create the variables and constraints for the neural network in Pyomo\n",
    "m.reformer.build_formulation(ReluBigMFormulation(net))\n",
    "\n",
    "# now add the objective and the constraints\n",
    "limonene_idx = outputs.index('Limonene')\n",
    "m.obj = pyo.Objective(expr=m.reformer.outputs[limonene_idx], sense=pyo.maximize)\n",
    "\n",
    "# now solve the optimization problem (this may take some time)\n",
    "solver = pyo.SolverFactory('cplex')\n",
    "status = solver.solve(m, tee=False)\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    print(f'{inputs[i]}:', pyo.value(m.reformer.inputs[i]))\n",
    "\n",
    "print('Limonene: ', pyo.value(m.reformer.outputs[limonene_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
